{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Dependecy Imports</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
<<<<<<< HEAD
    "import pickle\n",
    "from joblib import dump, load\n",
    "from sklearn.model_selection import train_test_split\n"
=======
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report"
>>>>>>> parent of 06ed8b2 (final)
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Import data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>An OBAMA “LOW LEVEL OFFENDER” Gets Early Relea...</td>\n",
       "      <td>According to the mother of the 35 year old mur...</td>\n",
       "      <td>politics</td>\n",
       "      <td>Mar 4, 2016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Myriad of ways the CIA tried (and failed) to a...</td>\n",
       "      <td>RTThe father of the Cuban Revolution remains u...</td>\n",
       "      <td>Middle-east</td>\n",
       "      <td>November 27, 2016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U.S. belatedly begins to comply with Russia sa...</td>\n",
       "      <td>WASHINGTON (Reuters) - The U.S. State Departme...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>October 26, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Literally Just Said He’d Take Guns Away...</td>\n",
       "      <td>In discussing shootings and crime in the first...</td>\n",
       "      <td>News</td>\n",
       "      <td>September 26, 2016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>There’s Something Hokey About Ted</td>\n",
       "      <td>21st Century Wire says At some point, the poli...</td>\n",
       "      <td>US_News</td>\n",
       "      <td>March 21, 2016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44893</th>\n",
       "      <td>ASPIRATIONS: Young Chinese seize the day, seiz...</td>\n",
       "      <td>(Reuters) - The world is this generation s oys...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>October 17, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44894</th>\n",
       "      <td>HILARIOUS! Look Who LIBERAL Middlebury Profess...</td>\n",
       "      <td>Two weeks ago at Middlebury College, Charles M...</td>\n",
       "      <td>politics</td>\n",
       "      <td>Mar 14, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44895</th>\n",
       "      <td>Senate to vote later on Wednesday to work with...</td>\n",
       "      <td>WASHINGTON (Reuters) - U.S. Senate Majority Le...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 6, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44896</th>\n",
       "      <td>Trump pulls nearly even with Clinton after Rep...</td>\n",
       "      <td>NEW YORK (Reuters) - Republican presidential ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>July 22, 2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44897</th>\n",
       "      <td>Kenyan president says repeat election must be ...</td>\n",
       "      <td>NAIROBI (Reuters) - Kenya s repeat presidentia...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>September 21, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44898 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "0      An OBAMA “LOW LEVEL OFFENDER” Gets Early Relea...   \n",
       "1      Myriad of ways the CIA tried (and failed) to a...   \n",
       "2      U.S. belatedly begins to comply with Russia sa...   \n",
       "3       Trump Literally Just Said He’d Take Guns Away...   \n",
       "4                      There’s Something Hokey About Ted   \n",
       "...                                                  ...   \n",
       "44893  ASPIRATIONS: Young Chinese seize the day, seiz...   \n",
       "44894  HILARIOUS! Look Who LIBERAL Middlebury Profess...   \n",
       "44895  Senate to vote later on Wednesday to work with...   \n",
       "44896  Trump pulls nearly even with Clinton after Rep...   \n",
       "44897  Kenyan president says repeat election must be ...   \n",
       "\n",
       "                                                    text       subject  \\\n",
       "0      According to the mother of the 35 year old mur...      politics   \n",
       "1      RTThe father of the Cuban Revolution remains u...   Middle-east   \n",
       "2      WASHINGTON (Reuters) - The U.S. State Departme...  politicsNews   \n",
       "3      In discussing shootings and crime in the first...          News   \n",
       "4      21st Century Wire says At some point, the poli...       US_News   \n",
       "...                                                  ...           ...   \n",
       "44893  (Reuters) - The world is this generation s oys...     worldnews   \n",
       "44894  Two weeks ago at Middlebury College, Charles M...      politics   \n",
       "44895  WASHINGTON (Reuters) - U.S. Senate Majority Le...  politicsNews   \n",
       "44896   NEW YORK (Reuters) - Republican presidential ...  politicsNews   \n",
       "44897  NAIROBI (Reuters) - Kenya s repeat presidentia...     worldnews   \n",
       "\n",
       "                      date  label  \n",
       "0              Mar 4, 2016      0  \n",
       "1        November 27, 2016      0  \n",
       "2        October 26, 2017       1  \n",
       "3       September 26, 2016      0  \n",
       "4           March 21, 2016      0  \n",
       "...                    ...    ...  \n",
       "44893    October 17, 2017       1  \n",
       "44894         Mar 14, 2017      0  \n",
       "44895    December 6, 2017       1  \n",
       "44896       July 22, 2016       1  \n",
       "44897  September 21, 2017       1  \n",
       "\n",
       "[44898 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real = pd.read_csv(\"./data/True.csv\")\n",
    "fake = pd.read_csv(\"./data/Fake.csv\")\n",
    "\n",
    "# add label \"0\" for fake \"1\" for real\n",
    "\n",
    "real[\"label\"] = 1\n",
    "fake[\"label\"] = 0\n",
    "\n",
    "# combine and shuffle data\n",
    "\n",
    "data = pd.concat([fake, real], axis=0)\n",
    "data = data.sample(frac=1)\n",
    "data.reset_index(inplace=True) \n",
    "data.drop([\"index\"], axis=1, inplace=True) \n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Cleaning and Preprocessing</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44898, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'https?://S+ | www\\.\\S+', '', text)\n",
    "    text = re.sub(r'\\n', '', text)\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    words=[]\n",
    "    for i in text:\n",
    "        if i not in string.punctuation:\n",
    "            words.append(i)\n",
    "    return ''.join(words)\n",
    "    \n",
    "data['title'] = data['title'].apply(clean_text)\n",
    "data['text'] = data['text'].apply(clean_text)\n",
    "\n",
    "data.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Test train split</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = data[\"text\"], data[\"label\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "code",
   "execution_count": 12,
=======
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Vectorize text for model training</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words = \"english\", max_df =0.7)\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
>>>>>>> parent of 06ed8b2 (final)
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "244200"
      ]
     },
     "execution_count": 12,
=======
       "['server/vectorizer.pkl']"
      ]
     },
     "execution_count": 6,
>>>>>>> parent of 06ed8b2 (final)
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "# count unique words\n",
    "word_count = {}\n",
    "word_index = 1\n",
    "for text in data[\"text\"]:\n",
    "    for word in text.split():\n",
    "        if word not in word_count:\n",
    "            word_count[word] = word_index\n",
    "            word_index += 1\n",
    "\n",
    "len(word_count)            "
=======
    "#save vectorizer\n",
    "joblib.dump(vectorizer, 'server/vectorizer.pkl')"
>>>>>>> parent of 06ed8b2 (final)
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "<h2>Preprocess Data</h2>"
=======
    "<h2> Train and evaluate classifier models</h2>"
>>>>>>> parent of 06ed8b2 (final)
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Assuming 'X_train' and 'X_test' contain the text data for training and testing respectively\n",
    "\n",
    "# Define parameters\n",
    "vocab_size = len(word_count)  # Adjust this based on your dataset and vocabulary size requirements\n",
    "max_length = 100    # Adjust this based on the maximum sequence length you want to use\n",
    "\n",
    "# Tokenize text data\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "with open('tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "# Convert text data to sequences of integers\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "# Now, X_train_padded and X_test_padded contain the tokenized and padded sequences ready for training and testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Compile LSTM</h2>"
=======
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score"
>>>>>>> parent of 06ed8b2 (final)
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 21,
=======
   "execution_count": 8,
>>>>>>> parent of 06ed8b2 (final)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 100, 100)          24420000  \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 64)                42240     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24462305 (93.32 MB)\n",
      "Trainable params: 24462305 (93.32 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Define vocabulary size and maximum sequence length\n",
    "vocab_size = len(word_count)\n",
    "max_length = 100\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=100, input_length=max_length))\n",
    "model.add(LSTM(units=64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "print(model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1123/1123 [==============================] - 110s 97ms/step - loss: 0.2366 - accuracy: 0.9052 - val_loss: 0.3000 - val_accuracy: 0.8712\n",
      "Epoch 2/5\n",
      "1123/1123 [==============================] - 115s 102ms/step - loss: 0.1816 - accuracy: 0.9354 - val_loss: 0.2668 - val_accuracy: 0.9192\n",
      "Epoch 3/5\n",
      "1123/1123 [==============================] - 116s 103ms/step - loss: 0.0615 - accuracy: 0.9833 - val_loss: 0.0405 - val_accuracy: 0.9893\n",
      "Epoch 4/5\n",
      "1123/1123 [==============================] - 109s 97ms/step - loss: 0.0644 - accuracy: 0.9781 - val_loss: 0.0281 - val_accuracy: 0.9914\n",
      "Epoch 5/5\n",
      "1123/1123 [==============================] - 108s 96ms/step - loss: 0.0193 - accuracy: 0.9940 - val_loss: 0.0246 - val_accuracy: 0.9941\n"
     ]
    }
   ],
   "source": [
    "#train model\n",
    "history = model.fit(X_train_padded, y_train, epochs=5, batch_size=32, validation_data=(X_test_padded, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281/281 [==============================] - 2s 8ms/step - loss: 0.0246 - accuracy: 0.9941\n",
      "Test Accuracy: 0.9940980076789856\n"
=======
      "Training LogisticRegression...\n",
      "\n",
      " LogisticRegression \n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98      4670\n",
      "           1       0.98      0.98      0.98      4310\n",
      "\n",
      "    accuracy                           0.98      8980\n",
      "   macro avg       0.98      0.98      0.98      8980\n",
      "weighted avg       0.98      0.98      0.98      8980\n",
      "\n",
      "Training DecisionTreeClassifier...\n",
      "\n",
      " DecisionTreeClassifier \n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      4670\n",
      "           1       1.00      1.00      1.00      4310\n",
      "\n",
      "    accuracy                           1.00      8980\n",
      "   macro avg       1.00      1.00      1.00      8980\n",
      "weighted avg       1.00      1.00      1.00      8980\n",
      "\n",
      "Training RandomForestClassifier...\n",
      "\n",
      " RandomForestClassifier \n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      4670\n",
      "           1       0.98      0.99      0.98      4310\n",
      "\n",
      "    accuracy                           0.99      8980\n",
      "   macro avg       0.99      0.99      0.99      8980\n",
      "weighted avg       0.99      0.99      0.99      8980\n",
      "\n",
      "Training LinearSVC...\n"
>>>>>>> parent of 06ed8b2 (final)
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test_padded, y_test)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "#save model\n",
    "model.save(\"model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
=======
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
>>>>>>> parent of 06ed8b2 (final)
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "1\n",
      "1/1 [==============================] - 0s 14ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.8873234]], dtype=float32)"
      ]
     },
     "execution_count": 31,
=======
      "\n",
      " LinearSVC \n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00      4670\n",
      "           1       0.99      1.00      0.99      4310\n",
      "\n",
      "    accuracy                           1.00      8980\n",
      "   macro avg       1.00      1.00      1.00      8980\n",
      "weighted avg       1.00      1.00      1.00      8980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifiers=[\n",
    "    LogisticRegression(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    LinearSVC()\n",
    "]\n",
    "for classifier in classifiers:\n",
    "    classifier_name=classifier.__class__.__name__\n",
    "    print(f\"Training {classifier_name}...\")\n",
    "    classifier.fit(X_train_vectorized, y_train)\n",
    "    y_pred=classifier.predict(X_test_vectorized)\n",
    "    accuracy=accuracy_score(y_test, y_pred)\n",
    "    print(f\"\\n {classifier_name} \\n \")\n",
    "    print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['server/model.pkl']"
      ]
     },
     "execution_count": 9,
>>>>>>> parent of 06ed8b2 (final)
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "#sample prediction\n",
    "sample = data[\"text\"][13]\n",
    "label = data[\"label\"][13]\n",
    "\n",
    "print(label)\n",
    "new_text_sequence = tokenizer.texts_to_sequences([sample])  # Assuming 'tokenizer' is the tokenizer used during training\n",
    "new_text_padded = pad_sequences(new_text_sequence, maxlen=max_length)  # Assuming 'max_length' is the maximum sequence length used during training\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(new_text_padded)\n",
    "predictions"
=======
    "from joblib import dump, load\n",
    "model = classifiers[3]\n",
    "dump(model, 'server/model.pkl') "
>>>>>>> parent of 06ed8b2 (final)
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
