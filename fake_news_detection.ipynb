{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Dependecy Imports</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "from joblib import dump, load\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Import data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LOVE HIM OR HATE HIM…HERE ARE 7 WAYS TRUMP HEL...</td>\n",
       "      <td>The GOP doesn t know it yet, but they need Tru...</td>\n",
       "      <td>politics</td>\n",
       "      <td>Jul 25, 2015</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Brazil detains Italian fugitive Battisti leavi...</td>\n",
       "      <td>BRASILIA (Reuters) - Brazilian highway police ...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>October 4, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hacking Electronic Voting Machines is Easy</td>\n",
       "      <td>21st Century Wire says It s the big dirty sec...</td>\n",
       "      <td>Middle-east</td>\n",
       "      <td>November 6, 2016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DEMOCRAT LAWMAKER Puts Forth Bill Requiring Wi...</td>\n",
       "      <td>Leave it to a Democrat to waste everyone s tim...</td>\n",
       "      <td>politics</td>\n",
       "      <td>Feb 17, 2016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WAS MICHELLE OBAMA In On Beyonce’s Cop-Hating ...</td>\n",
       "      <td>In an interview that is a tradition before the...</td>\n",
       "      <td>politics</td>\n",
       "      <td>Feb 8, 2016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44893</th>\n",
       "      <td>Trump Considering Incredibly Ironic Choice Fo...</td>\n",
       "      <td>Another day in the Trump Administration means ...</td>\n",
       "      <td>News</td>\n",
       "      <td>January 26, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44894</th>\n",
       "      <td>Chicago Police Dept. Points Out TWO Huge Flaw...</td>\n",
       "      <td>Trump s wild lies continue unabated as he cont...</td>\n",
       "      <td>News</td>\n",
       "      <td>August 25, 2016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44895</th>\n",
       "      <td>Four Serbs get suspended sentences for 2008 at...</td>\n",
       "      <td>BELGRADE (Reuters) - Four Serbian men got susp...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>November 14, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44896</th>\n",
       "      <td>PICTURE OF Cop Walking Son To School On Day He...</td>\n",
       "      <td>So what s worse, Barack Obama remaining silent...</td>\n",
       "      <td>left-news</td>\n",
       "      <td>Jul 29, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44897</th>\n",
       "      <td>Trump praises 'productive' China talks, but to...</td>\n",
       "      <td>BEIJING (Reuters) - U.S. President Donald Trum...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>November 10, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44898 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "0      LOVE HIM OR HATE HIM…HERE ARE 7 WAYS TRUMP HEL...   \n",
       "1      Brazil detains Italian fugitive Battisti leavi...   \n",
       "2             Hacking Electronic Voting Machines is Easy   \n",
       "3      DEMOCRAT LAWMAKER Puts Forth Bill Requiring Wi...   \n",
       "4      WAS MICHELLE OBAMA In On Beyonce’s Cop-Hating ...   \n",
       "...                                                  ...   \n",
       "44893   Trump Considering Incredibly Ironic Choice Fo...   \n",
       "44894   Chicago Police Dept. Points Out TWO Huge Flaw...   \n",
       "44895  Four Serbs get suspended sentences for 2008 at...   \n",
       "44896  PICTURE OF Cop Walking Son To School On Day He...   \n",
       "44897  Trump praises 'productive' China talks, but to...   \n",
       "\n",
       "                                                    text       subject  \\\n",
       "0      The GOP doesn t know it yet, but they need Tru...      politics   \n",
       "1      BRASILIA (Reuters) - Brazilian highway police ...     worldnews   \n",
       "2       21st Century Wire says It s the big dirty sec...   Middle-east   \n",
       "3      Leave it to a Democrat to waste everyone s tim...      politics   \n",
       "4      In an interview that is a tradition before the...      politics   \n",
       "...                                                  ...           ...   \n",
       "44893  Another day in the Trump Administration means ...          News   \n",
       "44894  Trump s wild lies continue unabated as he cont...          News   \n",
       "44895  BELGRADE (Reuters) - Four Serbian men got susp...     worldnews   \n",
       "44896  So what s worse, Barack Obama remaining silent...     left-news   \n",
       "44897  BEIJING (Reuters) - U.S. President Donald Trum...  politicsNews   \n",
       "\n",
       "                     date  label  \n",
       "0            Jul 25, 2015      0  \n",
       "1        October 4, 2017       1  \n",
       "2        November 6, 2016      0  \n",
       "3            Feb 17, 2016      0  \n",
       "4             Feb 8, 2016      0  \n",
       "...                   ...    ...  \n",
       "44893    January 26, 2017      0  \n",
       "44894     August 25, 2016      0  \n",
       "44895  November 14, 2017       1  \n",
       "44896        Jul 29, 2017      0  \n",
       "44897  November 10, 2017       1  \n",
       "\n",
       "[44898 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real = pd.read_csv(\"./data/True.csv\")\n",
    "fake = pd.read_csv(\"./data/Fake.csv\")\n",
    "\n",
    "# add label \"0\" for fake \"1\" for real\n",
    "\n",
    "real[\"label\"] = 1\n",
    "fake[\"label\"] = 0\n",
    "\n",
    "# combine and shuffle data\n",
    "\n",
    "data = pd.concat([fake, real], axis=0)\n",
    "data = data.sample(frac=1)\n",
    "data.reset_index(inplace=True) \n",
    "data.drop([\"index\"], axis=1, inplace=True) \n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Cleaning and Preprocessing</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44898, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'https?://S+ | www\\.\\S+', '', text)\n",
    "    text = re.sub(r'\\n', '', text)\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    words=[]\n",
    "    for i in text:\n",
    "        if i not in string.punctuation:\n",
    "            words.append(i)\n",
    "    return ''.join(words)\n",
    "    \n",
    "data['title'] = data['title'].apply(clean_text)\n",
    "data['text'] = data['text'].apply(clean_text)\n",
    "\n",
    "data.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Test train split</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = data[\"text\"], data[\"label\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "244200"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count unique words\n",
    "word_count = {}\n",
    "word_index = 1\n",
    "for text in data[\"text\"]:\n",
    "    for word in text.split():\n",
    "        if word not in word_count:\n",
    "            word_count[word] = word_index\n",
    "            word_index += 1\n",
    "\n",
    "len(word_count)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Preprocess Data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Assuming 'X_train' and 'X_test' contain the text data for training and testing respectively\n",
    "\n",
    "# Define parameters\n",
    "vocab_size = len(word_count)  # Adjust this based on your dataset and vocabulary size requirements\n",
    "max_length = 100    # Adjust this based on the maximum sequence length you want to use\n",
    "\n",
    "# Tokenize text data\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "with open('./server/tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "# Convert text data to sequences of integers\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "# Now, X_train_padded and X_test_padded contain the tokenized and padded sequences ready for training and testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Compile LSTM</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 100)          24420000  \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                42240     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24462305 (93.32 MB)\n",
      "Trainable params: 24462305 (93.32 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Define vocabulary size and maximum sequence length\n",
    "vocab_size = len(word_count)\n",
    "max_length = 100\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=100, input_length=max_length))\n",
    "model.add(LSTM(units=64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "print(model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1123/1123 [==============================] - 627s 557ms/step - loss: 0.1648 - accuracy: 0.9494 - val_loss: 0.1156 - val_accuracy: 0.9732\n",
      "Epoch 2/5\n",
      "1123/1123 [==============================] - 138s 123ms/step - loss: 0.1332 - accuracy: 0.9619 - val_loss: 0.0721 - val_accuracy: 0.9834\n",
      "Epoch 3/5\n",
      "1123/1123 [==============================] - 107s 96ms/step - loss: 0.0673 - accuracy: 0.9847 - val_loss: 0.0554 - val_accuracy: 0.9888\n",
      "Epoch 4/5\n",
      "1123/1123 [==============================] - 112s 100ms/step - loss: 0.0362 - accuracy: 0.9920 - val_loss: 0.0497 - val_accuracy: 0.9889\n",
      "Epoch 5/5\n",
      "1123/1123 [==============================] - 114s 101ms/step - loss: 0.0377 - accuracy: 0.9906 - val_loss: 0.0337 - val_accuracy: 0.9918\n"
     ]
    }
   ],
   "source": [
    "#train model\n",
    "history = model.fit(X_train_padded, y_train, epochs=5, batch_size=32, validation_data=(X_test_padded, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281/281 [==============================] - 2s 8ms/step - loss: 0.0246 - accuracy: 0.9941\n",
      "Test Accuracy: 0.9940980076789856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test_padded, y_test)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "#save model\n",
    "model.save(\"model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1/1 [==============================] - 0s 14ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.8873234]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sample prediction\n",
    "sample = data[\"text\"][13]\n",
    "label = data[\"label\"][13]\n",
    "\n",
    "print(label)\n",
    "new_text_sequence = tokenizer.texts_to_sequences([sample])  # Assuming 'tokenizer' is the tokenizer used during training\n",
    "new_text_padded = pad_sequences(new_text_sequence, maxlen=max_length)  # Assuming 'max_length' is the maximum sequence length used during training\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(new_text_padded)\n",
    "predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
